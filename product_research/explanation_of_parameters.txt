РАЗБОР dialogue_simulator.py ДЛЯ ТИММЕЙТА: КАКИЕ ПАРАМЕТРЫ ЕСТЬ И ЗА ЧТО ОТВЕЧАЮТ

Что это за скрипт
- Это CLI-тренажёр “Оператор ↔ ИИ-клиент” поверх Ollama.
- Ты руками вводишь реплики оператора, а модель отвечает как “Клиент” согласно:
  1) архетипу (тип личности),
  2) сложности (уровень сопротивления/ловушки),
  3) продуктовой теме (free/rko/bank_card),
  4) правилам безопасности/формата (meta_guard, чистка текста и т.п.).
- Скрипт пишет:
  - историю диалога в logs/dialog_*.json
  - метрики по запросам в logs/metrics_*.jsonl (если не выключено)

============================================================
1) ОСНОВНАЯ КОМАНДА, КОТОРУЮ ОБЫЧНО ЗАПУСКАЕМ

Пример:
python trainer.py \
  --product free \
  --archetype friendly \
  --difficulty 1 \
  --model qwen2.5:7b-instruct-q4_K_M \
  --transport http \
  --warm-up

Что она означает:
- Запускаем интерактивный live-режим тренажёра.
- Клиент = архетип friendly, сложность 1.
- Тема = free (без конкретного банковского продукта/сценария).
- Модель в ollama = qwen2.5:7b-instruct-q4_K_M.
- Генерация идёт через HTTP API Ollama.
- Перед стартом делаем “прогрев” (warm-up), чтобы первая реальная генерация была быстрее/стабильнее.

============================================================
2) ПАРАМЕТРЫ CLI (argparse)

--model (str)
- Название модели в Ollama, которую будем дергать.
- Примеры: qwen2.5:7b-instruct-q4_K_M, qwen2.5:14b-instruct-q4_K_M
- Важно: имя должно существовать в `ollama list`.

--mode (choices=["live"])
- Сейчас поддержан только live.
- live = интерактив: вводишь “Оператор: …” в консоли → получаешь “Клиент: …”.

--product (str)
- Выбор “темы/сценария” диалога.
- В коде PRODUCTS:
  - free: свободная тема (без сценарных фактов)
  - rko: разговор про РКО (счет, комиссии, обслуживание)
  - bank_card: разговор про бизнес-карту (лимиты, комиссии, безопасность)
- Как влияет:
  - В system prompt добавляется “Контекст/Факты/Цель клиента”, если product != free.
  - Это помогает модели держать релевантный контент.

--archetype (str)
- Тип личности клиента (ARCHETYPES):
  - novice: новичок, просит простыми словами, не умничать
  - skeptic: скептик, требует цифры, ищет подвох
  - busy_owner: занятой предприниматель, коротко и по делу
  - friendly: дружелюбный, вежливо, но считает деньги
- Как влияет:
  - В system prompt вставляются personality/speech_style/default_goal/taboos.
  - Также есть особое правило инициативы для novice (см. ниже).

--difficulty (str: "1"/"2"/"3"/"4")
- Уровень сопротивления/сложности (DIFFICULTY):
  - 1 лёгкий: низкое сопротивление, ловушек нет
  - 2 нормальный: среднее
  - 3 сложный: сопротивление высокое, ловушки включены
  - 4 очень сложный: очень высокое, ловушки включены
- Как влияет:
  - В system prompt записывается сопротивление/частота вопросов/ловушки (подсказка модели).

--timeout (int, default 90)
- Таймаут на ОДНУ генерацию ответа (ollama http/cli).
- Если истек — возвращаем fallback-ответ клиента и логируем TIMEOUT.

--max-turns (int, default 6)
- Сколько последних “ходов” из диалога передаём модели (ограничение истории).
- Реально используется в make_prompt → _select_history_by_budget.

--context-budget (int, default 650)
- “бюджет токенов” на историю в промпте.
- Это НЕ ollama num_ctx. Это наш внутренний лимит: сколько “примерно токенов” истории мы помещаем.
- Приоритет: берём хвост из max-turns, но если бюджет переполняется — режем ещё.

--max-sentences (int, default 5)
- Ограничение на число предложений в ответе клиента после чистки.
- Делает ответы короткими (1–5 предложений).

--reply-max-chars (int, default 320)
- Ограничение на длину ответа клиента в символах.
- Режет аккуратно “по границе предложения” (._trim_to_sentence_boundary), чтобы не обрубать мысль на середине.

--retries (int, default 2)
- Сколько “повторных попыток” сделать, если ответ модели плохой:
  - сырое содержимое с мусором (NON_RU_RAW)
  - после чистки всё равно мусор (NON_RU)
  - мета-утечка (META_GUARD)
  - role swap (ROLE_SWAP — клиент начал спрашивать про “вы/у вас?”)
  - пустой ответ (NO_REPLY)
- Если все попытки провалились — fallback.

--debug (flag)
- Печатает больше служебного:
  - ping Ollama, ошибки, fallback to cli, детали ошибок.

--turn-limit (int, default 30)
- Лимит количества ходов оператора за сессию.
- Если дошли до turn-limit — тренажёр остановит диалог автоматически.

Параметры генерации (ollama options):
--num-predict (int, default 120)
- Максимум токенов, которые модель сгенерит в ответ.
- Аналог “max tokens”.

--temperature (float, default 0.6)
- Творческость/случайность.
- Ниже = стабильнее/более шаблонно; выше = разнообразнее, но риск мусора/ухода в роль.

--top-p (float, default 0.9)
- Nucleus sampling. Ограничение по суммарной вероятности.
- Обычно: 0.8–0.95.

--repeat-penalty (float, default 1.1)
- Штраф за повторы.
- Полезно против “зацикливания” и повторения тех же фраз.

Транспорт и Ollama:
--transport (choices=["auto","http","cli"], default "http")
- http: всегда бьём по HTTP API Ollama (http://localhost:11434/api/generate)
- cli: всегда через `ollama run <model>`
- auto: пытаемся http, если не получилось — падаем на cli

--ollama-url (str, default "http://localhost:11434")
- База URL для Ollama API.
- Важно, если Ollama не на localhost или другой порт.

Защита от мета/утечек:
--no-meta-guard (flag)
- Если включить флаг, то meta_guard выключается.
- meta_guard = набор проверок, которые заставляют “плохие” ответы ретраиться:
  - is_meta_or_role_leak: “как клиент”, “правила”, “prompt”, “Оператор:” и т.п.
  - is_role_swap: “вопросы во 2-м лице” (“сколько у вас…?”)
- Обычно держим meta_guard ON.

Ускорители/контекст Ollama:
--keep-alive (default "15m")
- Параметр Ollama, чтобы модель “не выгружалась” из памяти некоторое время.
- Уменьшает лаги между запросами в одной/нескольких сессиях.

--num-ctx (int, default 1024)
- Реальный контекст модели в Ollama (сколько токенов она может держать).
- Важно: это отдельная штука от нашего --context-budget.
  - context_budget ограничивает, сколько истории МЫ положим.
  - num_ctx ограничивает, сколько модель вообще способна прочитать.

--stop (list of strings)
- Список “стоп-триггеров”, по которым модель должна прекращать генерацию.
- По умолчанию стопим, если модель начала писать префиксы:
  "\nM:", "\nОператор:", "\nКлиент:", и т.д.
- Важно: специально НЕ добавляем "\nC:" чтобы не обрубить корректный ответ клиента (в коде это комментарий).

Warm-up:
--warm-up (flag)
- Включает прогрев модели перед началом диалога.
- Делается маленький запрос: “Ответь одним словом: ок.”

--warm-up-timeout (int, default 120)
- Таймаут именно на warm-up запрос.

--warm-up-tokens (int, default 2)
- num_predict для warm-up (токенов очень мало, чтобы прогрев был быстрый).

Метрики:
--no-metrics (flag)
- Если включить — метрики не пишутся, и metrics_path не создаётся.
- Иначе:
  - по каждому запросу пишется JSON-строка в logs/metrics_*.jsonl
  - в конце печатается summary: latency avg/p50, tokens/sec и т.д.

============================================================
3) АРХЕТИПЫ И “ЧТО ЭТО ЗНАЧИТ В РЕАЛЬНОМ ДИАЛОГЕ”

ARCHETYPES:
- novice (Новичок)
  - ожидаем: “простыми словами”, может путаться, без агрессии
  - важно: у novice усиленное правило инициативы:
    - “вообще не задавай встречных вопросов менеджеру”
    - если не понял: “Не понял, поясните простыми словами” (но без “у вас/вы?”)
- skeptic (Скептик)
  - ожидаем: “покажите цифры”, коротко, без дружелюбия
  - не должен быстро соглашаться
- busy_owner (Занятой предприниматель)
  - ожидаем: “коротко”, “давайте тезисы”, раздражение если много воды
- friendly (Дружелюбный)
  - ожидаем: вежливо, готов обсуждать, но считает деньги

DIFFICULTY:
- Подсказка модели про сопротивление и ловушки.
- “ловушки=да” (3–4) — модель должна чаще проверять, сомневаться, давить на конкретику.

============================================================
4) SYSTEM PROMPT: КАК СКЛЕИВАЕТСЯ И ПОЧЕМУ ЭТО ВАЖНО

build_system_prompt() собирает “инструкцию для модели”:
- “Ты — ИИ-клиент. Отвечай ТОЛЬКО как клиент.”
- “Язык: только русский.”
- “Английский допускается только для брендов (Google Sheets, Excel, CRM).”
- “Формат: 1–5 коротких предложений, без списков.”
- “Правило инициативы” (разное для novice и остальных).
- Жёсткий бан на вопросы менеджеру во 2-м лице (сколько у вас…, какие у вас…).
- Параметры личности (архетип): характер, стиль речи, цель, табу.
- Параметры сложности.
- + если product != free → добавляются контекст/факты/цель по продукту.

============================================================
5) КАК ФОРМИРУЕТСЯ ПРОМПТ НА КАЖДЫЙ ХОД

make_prompt(system_prompt, conversation, max_turns, budget_tokens):
- Берёт историю диалога (хвост), но с ограничениями:
  - max_turns: не больше N последних реплик
  - context_budget: не больше N “примерных токенов” на историю
- Роли в промпте:
  - M: реплики оператора (manager)
  - C: реплики клиента (client)
- В конце добавляется “C:” — модель должна продолжить как клиент.

============================================================
6) ЗАЩИТЫ (GUARDS): ЗАЧЕМ ОНИ И ЧТО СЧИТАЮТ “ПЛОХИМ ОТВЕТОМ”

После генерации скрипт проверяет:
1) raw_has_non_ru_en_garbage(raw)
   - если в сыром ответе есть неразрешённые символы (арабский/китайский/ломаный мусор) → retry

2) clean_reply()
   - чистит префиксы ролей, буллеты, нормализует кавычки/тире
   - удаляет “левые” символы (но оставляет RU/EN и базовую пунктуацию)
   - режет до max_sentences и reply_max_chars

3) has_non_ru_en_garbage(reply)
   - если и после чистки остался мусор → retry

4) is_meta_or_role_leak(reply) (если meta_guard включён)
   - если модель начала говорить про “правила/инструкцию/system/prompt”
   - если ответ содержит “Оператор:” / “Manager:” и т.п.
   - если есть перевод строки (в коде запрещено, чтобы не было списков/многострочного вывода)
   → retry

5) is_role_swap(reply) (если meta_guard включён)
   - если клиент начал задавать вопросы менеджеру “во 2-м лице” (сколько у вас…, скажите…, подскажите…)
   → retry

6) is_repeat_reply(last_client_reply, reply)
   - если новый ответ почти повторяет прошлый (Jaccard >= 0.85)
   - не ретраит, но печатает предупреждение “Клиент повторяется”.

Если все ретраи исчерпаны → fallback-ответ (_fallback_client_reply).

============================================================
7) WARM-UP: ЧТО ЭТО И ЗАЧЕМ

warm_up() делает короткую генерацию перед началом диалога:
- Это “прогрев” модели (загрузка в память, создание кэшей, чтобы первая реальная реплика не была медленной).
- Запрос: “Ответь одним словом: ок.”
- temp=0, top_p=1, repeat_penalty=1 — чтобы ответ был максимально детерминированный.
- Работает через http или cli в зависимости от --transport.

============================================================
8) ЛОГИ: ЧТО СОХРАНЯЕТСЯ И ГДЕ

1) Диалог:
logs/dialog_{product}_{archetype}_L{difficulty}_{ts}.json

Формат:
{
  "ts": "...",
  "model": "...",
  "product": "...",
  "archetype": "...",
  "difficulty": "...",
  "turns": [
    {"role": "manager", "text": "..."},
    {"role": "client", "text": "..."},
    ...
  ]
}

2) Метрики (если не --no-metrics):
logs/metrics_{product}_{archetype}_L{difficulty}_{ts}.jsonl

Одна строка = один запрос к модели, примерно:
{
  "ts": ...,
  "model": ...,
  "transport": "http",
  "latency_total_s": ...,
  "latency_model_s": ...,
  "in_tokens": ...,
  "out_tokens": ...,
  "tps": ...,
  "err_reason": null | "TIMEOUT" | "HTTP_ERROR" | "NON_RU" | ...,
  "attempt": 1/2/3,
  ...
}

И в конце печатается агрегат:
- запросов / ok / timeout / errors
- latency avg/p50/min/max
- in/out tokens total
- tokens/sec avg/p50

============================================================
9) РЕКОМЕНДУЕМЫЕ “ПРЕСЕТЫ” ДЛЯ ТЕСТОВ (ЧТО ИМЕННО ГОНЯТЬ)

Быстро прогнать “все архетипы” одной моделью:
1) friendly L1:
python trainer.py --product free --archetype friendly --difficulty 1 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

2) busy_owner L2:
python trainer.py --product free --archetype busy_owner --difficulty 2 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

3) skeptic L3:
python trainer.py --product free --archetype skeptic --difficulty 3 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

4) novice L2:
python trainer.py --product free --archetype novice --difficulty 2 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

Отдельно по продуктам (если хотите сценарность):
- rko:
python trainer.py --product rko --archetype skeptic --difficulty 3 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

- bank_card:
python trainer.py --product bank_card --archetype friendly --difficulty 2 --model qwen2.5:7b-instruct-q4_K_M --transport http --warm-up

============================================================
10) КОРОТКО: КАК ПОНЯТЬ, ЧТО “СЛОМАЛОСЬ”

Красные флаги в диалогах:
- Клиент пишет “Оператор:” / “Менеджер:” → role leak
- Клиент пишет мета: “как модель”, “инструкция”, “правила” → meta leak
- В ответах мусор/смешение: suхом, detallей, вremя → чистка/guard не поймали или модель пробила
- Клиент начинает спрашивать “у вас/вы” → role swap
- Клиент повторяет одно и то же → repeat

